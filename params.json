{"name":"Pml-assignment","tagline":"","body":"Practical Machine Learning Assignment\r\nPaul Robberson\r\n10/22/2015\r\n\r\nSummary\r\nWe are concerned with being able to predict whether the simple exercise of curling a light dumbbell is being done correctly or incorrectly.  A write-up of this experiment can be found at http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf\r\nSix participants were asked to perform 10 repetitions of a dumbbell curl first as specified by a weight-lifting trainer (although the exact specifications are not mentioned), then 10 each of 4 commonly mistaken ways of doing the exercise.  These are captured in the data in the variable “classe” according to the following descriptions: \r\n•\texactly according to the speciﬁcation (Class A)\r\n•\tthrowing the elbows to the front (Class B)\r\n•\tlifting the dumbbell only halfway (Class C)\r\n•\tlowering the dumbbell only halfway (Class D)\r\n•\tthrowing the hips to the front (Class E).\r\nFour Razor 9-degrees of freedom sensors were mounted on dumbbell, forearm, arm, and belt.  Documentation on these sensors can be found at: https://www.sparkfun.com/products/10736.  These sensors each contain 3 devices, each of which took their measurements in three dimensions (x, y, z).\r\n•\taccelerometer, measuring acceleration including gravity – “accel” in the data\r\n•\tgyroscope, measuring angular motion – “gyros” in the data\r\n•\tmagnetometer, measuring distance – “magnet” in the data\r\nData points were captured at 45hz in sliding time chunks ranging from .5 seconds to 2.5 seconds and saved in a file containing 19,622 observations of 160 variables.\r\nThe following is a description of building a predictive model that identifies the “classe” using a subset of the other variables.\r\nDescription\r\n Exploring the data by reading the presented data file, “pml-training.csv”, and using the View() command shows that many of the columns have nearly all “NA” or are blank.  \r\n>trn <- read.csv(\"pml-training.csv\")\r\nExamining the structure of trn, we find:\r\n> str(trn)\r\n'data.frame':\t19622 obs. of  160 variables\r\n\r\nThe use of the View() command gives a good spreadsheet-like table of the data, but the number of columns and rows are limited.  Therefore, multiple View()’s are needed.\r\n>View(trn[,1:30])\r\n>View(trn[,31:60])\r\nEtc.\r\n\r\nIt should be noted that the data set is small enough to be read by Excel and perused in its entirety.\r\nIt seems reasonable to eliminate the columns that are mostly empty or NA.  Further, there is no data dictionary, so a reasonable assumption is that the 36 basic readings (3 x,y,z readings, 3 sensors, 4 devices) are the primary ones to investigate, plus the outcome “classe”.\r\n\r\n>trndat <- select(trn, classe, starts_with(“gyro”), starts_with(“accel”), starts_with(“magnet”))\r\n\r\nThe resulting trndat data frame has 19,622 observations of 37 variables.\r\nThe “pml-testing.csv” supplied file has 20 observations without the outcome “classe”.  This is used to\r\ntest the model in the Prediction Assignment.  Therefore, to create cross validation the trndat data frame is partitioned into .75/.25 training/testing sets (dattrn and dattst).  \r\n\r\n> trnindex <- createDataPartition(y=trndat$classe, p=0.75, list=FALSE)\r\n> dattrn <- trndat[trnindex,]\r\n> str(dattrn)\r\n'data.frame':\t14718 obs. of  37 variables\r\n\r\n> dattst <- trndat[-trnindex,]\r\n> str(dattst)\r\n'data.frame':\t4904 obs. of  37 variables:\r\n\r\nA Random Forest training of “classe” against the other 36 variables is created. \r\nmodfit <- train(dattrn$classe ~., method=\"rf\", data=dattrn)\r\n> modfit\r\nRandom Forest \r\n\r\n14718 samples\r\n   36 predictor\r\n    5 classes: 'A', 'B', 'C', 'D', 'E' \r\n\r\nNo pre-processing\r\nResampling: Bootstrapped (25 reps) \r\nSummary of sample sizes: 14718, 14718, 14718, 14718, 14718, 14718, ... \r\nResampling results across tuning parameters:\r\n\r\n  mtry  Accuracy    Kappa       Accuracy SD   Kappa SD   \r\n\r\n   2    0.9812954   0.9763349   0.002590328   0.003278947\r\n\r\n  19    0.9774804   0.9715090   0.002358977   0.002984152\r\n\r\n  36    0.9697265   0.9616962   0.003533743   0.004485859\r\n\r\nAccuracy was used to select the optimal model using  the largest value.\r\nThe final value used for the model was mtry = 2. \r\n\r\nBased on the Accuracy (.9812954) reported in the modfit model, we would expect an out-sample-error rate of 2% or a bit more.\r\nTo gain a cross validation, the modfit model was then used to predict the outcomes from the testing set, dattst. The table below shows these predictions vs actual outcomes.  The predictions on the testing set are .98878 accurate (4844/4904), just slightly better than the optimal model chosen by Random Forest.\r\n> pred <- predict(modfit, dattst)\r\n> table(pred, dattst$classe)\r\n    \r\npred    A     B    C     D     E\r\n\r\n   A 1393    10    0    1    0\r\n\r\n   B    0   935   11    0    0\r\n\r\n   C    0    4   844   24    0\r\n\r\n   D    1    0    0   776    5\r\n\r\n   E    1    0    0     3  896\r\n\r\n\r\nModfit was then used to predict the outcomes for the 20 observations in the supplied pml-testing.csv file.\r\n>tst <- read.csv(“pml-testing.csv”)\r\n>pred <- predict(modfit, tst)\r\nThe predictions were submitted and verified to be 100% correct.\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}